Hashing:
1.  HashTables can solve the problems for searching, insertion and sometimes deletion in O(1) time complexity.
2.  They are based on arrays and arrays have limited capicity and need copying to bigger array for expansion,
    Hashtables also face same issues.
3.  Hashmaps are bad if you want to visit elements in an order. you can only visit the element if you have its
    key value.
4.  In hashing KEYVALUES are transformed to ARRAYINDICES  using a HASH FUNCTION.
5.  Hash functions are not always required, sometimes you can use direct addressing.
6.  A simple hash function is big number % small range = small number.
7.  Load factor is the ratio of (Number of items in table/size of table).
8.  Load factor defines when HashTables need rehashing.

*****CASE STUDY: (From Data structures and Algorithms in java - Robert Lafore)

Direct Addressing:

A simple example is when we have a company of 1000 employees.
Where Some employee are added new while others leave.
When an employee leaves the company then we do not just go and delete him from the array. We keep his record.
Also when a new employee is added we can increase the array size.

In this case if a new employee joins who was 1001th in the company, how will we add him?
We have to expand the array.

This is called the direct addressing where we have a record of an employee directly mapped to an address in array.

*   Direct addressing is helpful when the data is not random(1000 employee ids-> 1000 records).

But what will happen if it was a dictionary?

Hashing:

Suppose we have a dictionary with 50k words and we want to save them in a data structure.
Words are english words like cats, dogs, deer etc

The challenge here is that these are character strings and our memory is a numeric location.

Suppose we want to add a word cats to the memory then we have to find a way to convert it to integer

1.  We can sum up the character positions like C(3) + A(1) + T(20) +S(19) = 43

    The issue with this approach is that if we have the largest word as zzzzzzzzzz then sum becomes = 260
    How will we be able to map 50000 words to only 260 locations. This worsens the problem.

2.  We can multiply with the power of 27 just like we do for decimal numbers 1234 = 1*10^3 + 2*10^2 + 3*10^1 + 4*10^0
    We do it because at every place there can be 10 numbers and if we take 2 digits then it makes 100 numbers 0-99

    So CATS become 3*27^3 + 1*27^2 + 20*27^1 + 19*27^0 = 60337

    *** The above calculation is done by a hash function that is generating a Key(Big number) for us and then we % this key
    with array size to get a hash value.

    For zzzzzzzzzz it will become more than 7,000,000,000,000 which gives us all the combinations of characters including
    the words like zzzdssdf which does not exist.

    The good part of this is that it covers all 50k words and more if added in future.

    Now the challenge is to map 50k to 7,000,000,000,000.

    Instead of mapping 50k we will take 100k as array size. and map all these 7,000,000,000,000 there. We keep it twice the size because
    many words can map to same location and we want our hashmap to perform better.

    Now when we have our numbers we need a function to map them.

    We usually use smallNumber = Key % smallRange(Make it prime to avoid clustering)
    arrayIndex = 500000 % 100000 = 0
    arrayIndex = 500001 % 100000 = 1
    ...
    ...

****The hashed value is the arrayIndex here where an actual value 500001 is hashed to arrayIndex.

/**********************************************************************************************************************/

Collisions:

Collisions happen when we try to compress and bigger range into smaller. Right now we have a range of 7,000,000,000,000
with only 50k as actual words that are being added to a 100k array. Ideally the overlap should not happen but it does.
Many words have the same hash and point to the same memory location that causes collisions.

The hashing performs best with data where there is no duplication.

Open Addressing:

When open addressing we find a new address for the item to be inserted in the same array. We usually do this using below 3 methods:

1.  Linear Probing:
    A.  INSERT: When items are inserted we calculate their hash values using a hash function and then we go to the hashed value and insert the item.
    B.  If the hashed value is already occupied we parse through the array to find an empty cell and insert it there.
    C.  FIND: When finding the value we do the same thing we just go to the location of the hashed value and try to locate the item there.
        If the item is not found at this location then we go to the next cell and keep going on until we find the value.
        The process is called PROBING. If we find an empty cell on our way then we return saying that the item was not found.
    D.  DELETE: If we are deleting an item then we do not just go to its cell and delete it but we go to the location and delete the value and insert
        a DEL or some value that cannot be our data.
    E.  Next time during a find if we see DEL then we just ignore it and search the next cell.
    F.  Next time during insert we consider DEL as an empty cell and we insert the item there.
    G.  Linear probing usually works fine but sometimes many values hash to the same position making the algorithm to add them to next vacant cells.
        ultimately creating long chain of values. These values cause longer insertion, deletion and find times. The problem is called CLUSTERING.
    H.  Duplicates can be added but the probing will return only the first value.

****What to do when hashmap becomes full:   Hashmap can become full. In this case we have to make the array double its size to occupy the new values
    THe problem here is that we cannot just copy older values to new array because the hash function depends on array size and will change.
    In this case we have to insert all the data from older map to the newer one using the new hash function. The procedure is called rehashing.
    For rehashing you have to create a new hashmap with a prime size. The new prime needs to be calculated as a part of rehashing process.

**  Linear probing has a problem of clustering where clusters grow bigger and bigger.

*** Load Factor = totalItems/ arraySize
    For optimal performance. We should do rehashing when loadfactor comes around 2/3 or above.

2.  Quadratic Probing:
    
    A.  Linear probing is prone to clustering. The clusters keep growing making insertion , find and deletion difficult.
    B.  Suppose some items are hashed to index X in linear probing then we save each item in index X, X+1, X+2...
        The issue here is that the items are forming a single large cluster this way.
    C.  In Quadratic probing we save the items at indexes X, X+1^2, X+2^2, X+3^2....
        This makes it X, X+1, X+4, X+9....This avoids the problem of PRIMARY CLUSTERING.
    D.  When quadratic probing finds a cluster it searches larger steps and keep increasing steps.
    E.  Always keep the array size prime.
    F.  The problem with quadratic probing is when all the keys hash to the same array location.
        Suppose this value is 7 and 10 keys hash there then they will probe X, X+1, X+4, X+9.... which will be very long probes.
        This problem is called SECONDARY CLUSTERING.

3.  Double Hashing:

    A.  The problem with Quadratic Probing is that steps that are produced are same for all keys that map to same location.
        every new key will have to parse all cells.
    B.  The way this can be improved is if the step size depends on the key.
    C.  To do this we use another hash function which determines the location for next key.
    D.  The new hash function should not be same as previous one and should never return 0.
    E.  The new hash function is usually like: stepSize = constant - (key % constant).
        Here constant is a prime and is smaller than array size.
        The constant determines the stepSize(it cannot be greater than constant so constant is kept smaller than array size)
